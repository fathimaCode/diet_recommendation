{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e731191-2fc5-4596-96c5-77b93ae7e776",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Loss: 1.2539039714336395\n",
      "Epoch 20/100, Loss: 1.23795349316597\n",
      "Epoch 30/100, Loss: 1.2240650898456573\n",
      "Epoch 40/100, Loss: 1.1971054473638534\n",
      "Epoch 50/100, Loss: 1.192634397006035\n",
      "Epoch 60/100, Loss: 1.1910223511219025\n",
      "Epoch 70/100, Loss: 1.194203799510002\n",
      "Epoch 80/100, Loss: 1.1909582775354386\n",
      "Epoch 90/100, Loss: 1.1910071989774704\n",
      "Epoch 100/100, Loss: 1.1909530151367187\n",
      "Model saved successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "C:\\Users\\binuo\\AppData\\Local\\Temp\\ipykernel_9032\\183361792.py:144: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('multi_class_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Status: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Define the Transformer Classification Model\n",
    "class TransformerClassificationModel(nn.Module):\n",
    "    def __init__(self, input_dim, d_model=64, nhead=4, num_encoder_layers=2, num_decoder_layers=2, num_classes=4):\n",
    "        super(TransformerClassificationModel, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, d_model)  # Embedding Layer\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, d_model))\n",
    "\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_encoder_layers)\n",
    "\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead)\n",
    "        self.decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=num_decoder_layers)\n",
    "\n",
    "        self.fc = nn.Linear(d_model, num_classes)  # Output layer for the number of classes\n",
    "        self.softmax = nn.Softmax(dim=1)  # Softmax for multi-class classification\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # Add positional encoding and apply embedding\n",
    "        src = self.embedding(src) + self.positional_encoding\n",
    "        tgt = self.embedding(tgt) + self.positional_encoding\n",
    "\n",
    "        # Ensure the input is 3D for permute (batch_size, d_model) -> (batch_size, seq_len=1, d_model)\n",
    "        src = src.unsqueeze(1) if src.dim() == 2 else src\n",
    "        tgt = tgt.unsqueeze(1) if tgt.dim() == 2 else tgt\n",
    "\n",
    "        # Ensure that inputs are 3D before permuting\n",
    "        if src.dim() == 4:\n",
    "            src = src.squeeze(1)  # Remove extra dimension if present\n",
    "        if tgt.dim() == 4:\n",
    "            tgt = tgt.squeeze(1)  # Remove extra dimension if present\n",
    "\n",
    "        # Permute to fit Transformer input format (seq_len=1, batch_size, d_model)\n",
    "        src = src.permute(1, 0, 2)  # (seq_len=1, batch_size, d_model)\n",
    "        tgt = tgt.permute(1, 0, 2)  # (seq_len=1, batch_size, d_model)\n",
    "\n",
    "        # Transformer encoding and decoding\n",
    "        memory = self.encoder(src)\n",
    "        output = self.decoder(tgt, memory)\n",
    "\n",
    "        # Reshape and apply final fully connected layer\n",
    "        output = output.permute(1, 0, 2).squeeze(1)\n",
    "\n",
    "        # Pass through the fully connected layer\n",
    "        output = self.fc(output)  # Now the output has shape (batch_size, num_classes)\n",
    "\n",
    "        # Apply softmax to get class probabilities\n",
    "        output = self.softmax(output)\n",
    "\n",
    "        return output  # Probability for each class\n",
    "\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('health_data.csv')\n",
    "\n",
    "# Filter out rows where Status contains invalid or unknown values\n",
    "valid_statuses = [\"Low\", \"Medium\", \"High\", \"Above High\"]\n",
    "data = data[data['Status'].isin(valid_statuses)]  # Keep only rows with valid statuses\n",
    "\n",
    "# Encode the Gender column using LabelEncoder\n",
    "label_encoder_gender = LabelEncoder()\n",
    "data['Gender'] = label_encoder_gender.fit_transform(data['Gender'])\n",
    "\n",
    "# Encode the Disease column\n",
    "label_encoder_disease = LabelEncoder()\n",
    "data['Disease'] = label_encoder_disease.fit_transform(data['Disease'])\n",
    "\n",
    "# Map the Status column to numerical values\n",
    "status_mapping = {\"Low\": 0, \"Medium\": 1, \"High\": 2, \"Above High\": 3}\n",
    "data['Status'] = data['Status'].map(status_mapping)\n",
    "\n",
    "# Select the feature and target columns\n",
    "X = data[['Gender', 'Age', 'Height (cm)', 'Weight (kg)', 'BMI', 'Disease']]\n",
    "y = data['Status']\n",
    "\n",
    "# Scale the feature columns\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the data into PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)  # Change to long for classification\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)  # Change to long for classification\n",
    "\n",
    "# Create PyTorch Datasets and DataLoaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Initialize the model\n",
    "input_dim = X_train.shape[1]  # Number of features in the input\n",
    "model = TransformerClassificationModel(input_dim)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Use CrossEntropyLoss for classification\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, inputs)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Compute loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'multi_class_model1.pth')\n",
    "print(\"Model saved successfully.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78d48308-89b7-433c-9774-3f9ca3673814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Status: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "C:\\Users\\binuo\\AppData\\Local\\Temp\\ipykernel_9032\\606346334.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('multi_class_model.pth'))\n"
     ]
    }
   ],
   "source": [
    "# Test with a single input value\n",
    "# Example single input value (ensure the format is correct)\n",
    "single_input = np.array([[0, 25, 180, 75, 40.1, 2]])  # Example input\n",
    "\n",
    "# Scale the input using the same scaler\n",
    "single_input_scaled = scaler.transform(single_input)\n",
    "\n",
    "# Convert the scaled input to a tensor\n",
    "single_input_tensor = torch.tensor(single_input_scaled, dtype=torch.float32)\n",
    "\n",
    "# Load the trained model for testing\n",
    "model = TransformerClassificationModel(input_dim)\n",
    "model.load_state_dict(torch.load('multi_class_model.pth'))\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Pass the input through the model\n",
    "with torch.no_grad():  # No gradients needed for testing\n",
    "    single_input_tensor = single_input_tensor.unsqueeze(0)  # Add batch dimension\n",
    "    output = model(single_input_tensor, single_input_tensor)  # Forward pass\n",
    "\n",
    "# Get the predicted class\n",
    "predicted_class = torch.argmax(output, dim=1)\n",
    "print(f'Predicted Status: {predicted_class.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80b35597-39f1-4729-8e6a-7eb28a7159ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Statuses for the batch: tensor([0, 3, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "C:\\Users\\binuo\\AppData\\Local\\Temp\\ipykernel_9032\\61995748.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('multi_class_model1.pth'))\n"
     ]
    }
   ],
   "source": [
    "# Example multiple inputs (ensure the format is correct)\n",
    "multiple_inputs = np.array([[1, 25, 180, 75, 23.1, 2], \n",
    "                            [0, 35, 165, 60, 22.0, 1], \n",
    "                            [1, 45, 170, 85, 29.4, 3]])  # Example input batch\n",
    "\n",
    "# Scale the inputs using the same scaler\n",
    "multiple_inputs_scaled = scaler.transform(multiple_inputs)\n",
    "\n",
    "# Convert the scaled inputs to a tensor\n",
    "multiple_inputs_tensor = torch.tensor(multiple_inputs_scaled, dtype=torch.float32)\n",
    "\n",
    "# Load the trained model for testing\n",
    "model = TransformerClassificationModel(input_dim)\n",
    "model.load_state_dict(torch.load('multi_class_model1.pth'))\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Pass the inputs through the model\n",
    "with torch.no_grad():  # No gradients needed for testing\n",
    "    multiple_inputs_tensor = multiple_inputs_tensor.unsqueeze(1)  # Add batch dimension\n",
    "    output = model(multiple_inputs_tensor, multiple_inputs_tensor)  # Forward pass\n",
    "\n",
    "# Get the predicted classes for the batch\n",
    "predicted_classes = torch.argmax(output, dim=1)\n",
    "print(f'Predicted Statuses for the batch: {predicted_classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd1a643e-aa13-45f3-ac08-770a72d1d89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Statuses for the batch: tensor([0, 3, 1, 1, 3, 0, 1, 1, 0, 3, 1, 3, 0, 1, 1, 3, 0, 1, 1, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "D:\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "C:\\Users\\binuo\\AppData\\Local\\Temp\\ipykernel_9032\\2001406042.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('multi_class_model1.pth'))\n"
     ]
    }
   ],
   "source": [
    "# Example of 20 inputs (samples) with features: [Gender, Age, Height (cm), Weight (kg), BMI, Disease]\n",
    "multiple_inputs = np.array([\n",
    "    [1, 25, 180, 75, 23.1, 2], \n",
    "    [0, 35, 165, 60, 22.0, 1], \n",
    "    [1, 45, 170, 85, 29.4, 3], \n",
    "    [0, 30, 175, 70, 22.9, 0], \n",
    "    [1, 50, 160, 55, 21.5, 1], \n",
    "    [0, 40, 185, 90, 26.3, 2], \n",
    "    [1, 28, 155, 50, 20.8, 3], \n",
    "    [0, 33, 178, 78, 24.6, 0], \n",
    "    [1, 22, 172, 68, 22.9, 2], \n",
    "    [0, 27, 168, 64, 22.7, 1], \n",
    "    [1, 55, 182, 88, 26.6, 0], \n",
    "    [0, 38, 162, 58, 22.1, 1], \n",
    "    [1, 47, 177, 80, 25.5, 2], \n",
    "    [0, 29, 174, 72, 23.8, 3], \n",
    "    [1, 34, 169, 66, 23.1, 0], \n",
    "    [0, 31, 170, 67, 23.2, 1], \n",
    "    [1, 53, 178, 83, 26.2, 2], \n",
    "    [0, 44, 164, 59, 21.9, 3], \n",
    "    [1, 26, 176, 74, 23.9, 0], \n",
    "    [0, 36, 167, 61, 22.0, 1]\n",
    "])\n",
    "\n",
    "# Scale the inputs using the same scaler\n",
    "multiple_inputs_scaled = scaler.transform(multiple_inputs)\n",
    "\n",
    "# Convert the scaled inputs to a tensor\n",
    "multiple_inputs_tensor = torch.tensor(multiple_inputs_scaled, dtype=torch.float32)\n",
    "\n",
    "# Load the trained model for testing\n",
    "model = TransformerClassificationModel(input_dim)\n",
    "model.load_state_dict(torch.load('multi_class_model1.pth'))\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Pass the inputs through the model\n",
    "with torch.no_grad():  # No gradients needed for testing\n",
    "    multiple_inputs_tensor = multiple_inputs_tensor.unsqueeze(1)  # Add batch dimension\n",
    "    output = model(multiple_inputs_tensor, multiple_inputs_tensor)  # Forward pass\n",
    "\n",
    "# Get the predicted classes for the batch\n",
    "predicted_classes = torch.argmax(output, dim=1)\n",
    "print(f'Predicted Statuses for the batch: {predicted_classes}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175db90d-d2c8-4d64-af4c-8f39b11df656",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
